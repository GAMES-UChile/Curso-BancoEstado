{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso BancoEstado-CMM: Aprendizaje de Máquinas Avanzado \n",
    "**Autores:** Taco de Wolff y Felipe Tobar  \n",
    "\n",
    "**Fecha:** 30 octubre, 2019\n",
    "\n",
    "\n",
    "# Módulo 5 - Modelos paramétricos y redes neuronales usando PyTorch\n",
    "**Contenidos**\n",
    "- ¿Qué es PyTorch y cómo se compara con NumPy?\n",
    "- Construir y entrenar una Red Neuronal\n",
    "\n",
    "## Introducción\n",
    "PyTorch es una librería basado en Python para computaciones escientíficas con el objetivo de reemplazar NumPy para utilizar el poder de GPU, y proveer una media de investigaciones aprendizaje profunda con velocidad y flexibilidad. También existe TensorFlow que proviene más o menos lo mismo, pero una de las principales diferencias entre TensorFlow y PyTorch es que TensorFlow genera grafos estaticos, que deben ser construidos por completo antes de evaluarlos, en cambio PyTorch presenta grafos dinámicos, pueden ser modificados y evaluados por nodo.\n",
    "\n",
    "Características de PyTorch:\n",
    "* Permite trabajo en tensores, similares a NumPy array pero permitiendo operaciones en la GPU.\n",
    "* Diferenciación automática para construir y entrenar modelos, en particular redes neuronales.\n",
    "* La gran diferencia con usar NumPy es que PyTorch (al igual que TensorFlow) construyen un grafo de computación, que luego es alimentado con los valores.\n",
    "\n",
    "Instalar con `conda install pytorch torchvision cpuonly -c pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores\n",
    "Un tensor de PyTorch es similar al `ndarray` de NumPy. Exploremos que son tensores con código. Podemos alocar un tensor sin initializar sus valores con `torch.empty(shape)`. Esta nos da un tensor con valores que estaban en el memoria antes y es necesario initializar todos valores nosostros mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También existen funciones que initializan los valores automaticamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con valores aleatoreas entre `0` y `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2407, 0.4107, 0.3400],\n",
       "        [0.9043, 0.4654, 0.8055],\n",
       "        [0.9965, 0.9968, 0.9355],\n",
       "        [0.4720, 0.7296, 0.7709],\n",
       "        [0.1129, 0.5934, 0.5383]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O con desde un areglo de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[5.5, 9, 3], [9.9, 0.2, 3]])\n",
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conección con NumPy\n",
    "Al fondo PyTorch y NumPy usan los mismos datos. Cambiar desde NumPy hasta PyTorch no duplica los datos, refieren a los mismos datos. Es decir que cambiar los datos en uno afectan los valores en el otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un areglo en NumPy, convertímoslo hasta PyTorch. Después cambiamos un valor en NumPy y veamos el cambio en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([  1.,   1.,   1., 100.,   1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np_areglo = np.ones(5)\n",
    "pt_tensor = torch.from_numpy(np_areglo)\n",
    "\n",
    "print(pt_tensor)\n",
    "\n",
    "np_areglo[3] = 100\n",
    "\n",
    "print(pt_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también el revés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[  1.   1.   1. 100.   1.]\n"
     ]
    }
   ],
   "source": [
    "pt_tensor = torch.ones(5)\n",
    "np_areglo = pt_tensor.numpy()\n",
    "\n",
    "print(np_areglo)\n",
    "\n",
    "pt_tensor[3] = 100\n",
    "\n",
    "print(np_areglo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones básicas con tensores\n",
    "Sumar dos tensores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1782, 1.4035, 1.9314],\n",
       "        [1.5287, 1.3101, 1.6226],\n",
       "        [1.9442, 1.7406, 1.0712],\n",
       "        [1.3767, 1.8617, 1.3836],\n",
       "        [1.2778, 1.6913, 1.4779]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que pasa arriba es que creemos un nuevo tensor con valores del sumo entre `x` y `y`. Cuando estemos en un bucle sumando los dos tensores por ejemplo, lo de arriba será muy ineficiente por que estamos alocando memoria de un tensor nuevo cada vez.\n",
    "\n",
    "PyTorch nos permite definir explícito que queremos usar el tensor `z` para la salida, evitando alocar más memoria. Usemos `torch.add()` con el parámetros `out` que define donde guardar los resultados de la operación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y =\n",
      " tensor([[1.5051, 1.0503, 1.6637],\n",
      "        [1.4969, 1.0521, 1.5821],\n",
      "        [1.1165, 1.6013, 1.7955],\n",
      "        [1.9918, 1.6178, 1.0880],\n",
      "        [1.5296, 1.9169, 1.5099]])\n",
      "\n",
      "2x + y =\n",
      " tensor([[2.5051, 2.0503, 2.6637],\n",
      "        [2.4969, 2.0521, 2.5821],\n",
      "        [2.1165, 2.6013, 2.7955],\n",
      "        [2.9918, 2.6178, 2.0880],\n",
      "        [2.5296, 2.9169, 2.5099]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "z = torch.empty(5, 3)\n",
    "\n",
    "torch.add(x, y, out=z)\n",
    "print(\"x + y =\\n\", z)\n",
    "\n",
    "torch.add(x, z, out=z)\n",
    "print(\"\\n2x + y =\\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cambiar tamaño\n",
    "Cambiar el tamaño del tensor usando `torch.view()`. Si tenemos un tensor con tamaño `(4,4)`, podemos cambiar la 'vista' al tensor y cambiar su tamaño. Dando un tamaño `-1` para una dimensión automaticamente usa el valor que es válido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4235e-01,  8.2155e-01,  2.6053e+00, -1.2334e+00],\n",
       "        [ 5.1302e-04, -9.4395e-01, -2.4674e-01, -1.4677e+00],\n",
       "        [-4.1857e-01, -5.5607e-02, -2.5504e-01,  4.8656e-01],\n",
       "        [-1.8836e-01,  3.4769e-01,  9.6786e-01,  1.1124e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(4, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.4235e-01,  8.2155e-01,  2.6053e+00, -1.2334e+00,  5.1302e-04,\n",
       "        -9.4395e-01, -2.4674e-01, -1.4677e+00, -4.1857e-01, -5.5607e-02,\n",
       "        -2.5504e-01,  4.8656e-01, -1.8836e-01,  3.4769e-01,  9.6786e-01,\n",
       "         1.1124e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4235e-01,  8.2155e-01],\n",
       "        [ 2.6053e+00, -1.2334e+00],\n",
       "        [ 5.1302e-04, -9.4395e-01],\n",
       "        [-2.4674e-01, -1.4677e+00],\n",
       "        [-4.1857e-01, -5.5607e-02],\n",
       "        [-2.5504e-01,  4.8656e-01],\n",
       "        [-1.8836e-01,  3.4769e-01],\n",
       "        [ 9.6786e-01,  1.1124e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4235e-01,  8.2155e-01],\n",
       "        [ 2.6053e+00, -1.2334e+00],\n",
       "        [ 5.1302e-04, -9.4395e-01],\n",
       "        [-2.4674e-01, -1.4677e+00],\n",
       "        [-4.1857e-01, -5.5607e-02],\n",
       "        [-2.5504e-01,  4.8656e-01],\n",
       "        [-1.8836e-01,  3.4769e-01],\n",
       "        [ 9.6786e-01,  1.1124e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(8, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.4235e-01,  8.2155e-01],\n",
       "         [ 2.6053e+00, -1.2334e+00]],\n",
       "\n",
       "        [[ 5.1302e-04, -9.4395e-01],\n",
       "         [-2.4674e-01, -1.4677e+00]],\n",
       "\n",
       "        [[-4.1857e-01, -5.5607e-02],\n",
       "         [-2.5504e-01,  4.8656e-01]],\n",
       "\n",
       "        [[-1.8836e-01,  3.4769e-01],\n",
       "         [ 9.6786e-01,  1.1124e+00]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(4, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplicaciones\n",
    "Por cada valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 24, 18])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([4, 3, 9])\n",
    "b = torch.tensor([1, 8, 2])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product entre dos vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(a, b) # a^T b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer product entre vectores da una matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 32,  8],\n",
       "        [ 3, 24,  6],\n",
       "        [ 9, 72, 18]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ger(a, b) # a b^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre un vector y una matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 16.,  13., -97.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([[2.0, 0.5], [1.0, -1.0], [-9.0, 4.0]])\n",
    "v = torch.tensor([9.0, -4.0])\n",
    "torch.mv(m, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre dos matrizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4544, 0.9667, 0.2806, 0.4770],\n",
       "        [0.8022, 1.4211, 0.5585, 0.7921],\n",
       "        [0.6241, 1.3195, 0.7368, 0.6820],\n",
       "        [0.5082, 0.6786, 0.1941, 0.4461],\n",
       "        [0.7949, 0.9010, 0.6169, 0.6922]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.rand(5, 3)\n",
    "m2 = torch.rand(3, 4)\n",
    "m3 = torch.mm(m1, m2) # tamaño 5x4\n",
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4544, 1.4211, 0.7368, 0.4461])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.diag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "El package the `Autograd` permite diferenciación automática para todas las operaciones con tensores. Especialmente util al realizar _Backpropagation_ en las redes neuronales.\n",
    "\n",
    "Para activarlo, al crear un tensor se debe fijar el atributo `.requires_grad` como `True` guarda todas las operaciones en dicho tensor, cuando terminas de realizar calculos (Por ejemplo aplicar capas de una red) se puede llamar a `.backward()` y tener todos los gradientes calculados automáticamente. Donde el gradiente del tensor queda acumulado en el atributo `.grad`\n",
    "\n",
    "Es este caso, tenemos\n",
    "$$A = \\begin{bmatrix}1&0&2\\end{bmatrix}$$\n",
    "$$B(A) = A * A$$\n",
    "$$C(B) = B * 2$$\n",
    "\n",
    "Según el _chain rule_ decimos\n",
    "$$\\frac{\\delta C}{\\delta A} = C'(B) * B'(A) = 2 * 2 A$$\n",
    "$$\\frac{\\delta C}{\\delta B} = C'(B) = 2$$\n",
    "\n",
    "La función `backward(x)` calcula la `grad` para todos los variables que tiene `requires_grad=True` o `retain_grad()`. El argumento `x` es la gradiente al frente de nuestro graph (más adelante de $C$). Normalmente tomamos una tensor unidad con tamaño de $C$ para `x`. Por ejemplo, $\\textrm{A.grad} = \\frac{\\delta C}{\\delta A} = 4\\cdot\\begin{bmatrix}1&0&2\\end{bmatrix} = \\begin{bmatrix}4&0&8\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[1., 0., 2.]], requires_grad=True)\n",
      "B: tensor([[1., 0., 4.]], grad_fn=<MulBackward0>)\n",
      "C: tensor([[2., 0., 8.]], grad_fn=<MulBackward0>)\n",
      "A.grad: tensor([[4., 0., 8.]])\n",
      "B.grad: tensor([[2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1,0,2]], dtype=torch.float, requires_grad=True) # calculamos el grad para A\n",
    "B = A * A\n",
    "C = B * 2\n",
    "\n",
    "B.retain_grad() # calculamos también el grad para B\n",
    "\n",
    "print('A:', A)\n",
    "print('B:', B)\n",
    "print('C:', C)\n",
    "\n",
    "C.backward(torch.ones(1, 3)) # pasamos un vector [1,1,1] para calcular el grad\n",
    "\n",
    "print('A.grad:', A.grad)\n",
    "print('B.grad:', B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos\n",
    "$$f = \\frac{1}{4}\\sum_j 3(x_j + 2)^2$$\n",
    ", thus\n",
    "$$\\begin{align}\n",
    "\\frac{\\delta f}{\\delta x_i} &= \\frac{1}{4}\\sum_j \\frac{\\delta z_j}{\\delta x_i}& \\\\\n",
    "&= \\frac{1}{4}\\sum_j \\frac{\\delta 3y_j^2}{\\delta x_i} & \\\\\n",
    "&= \\frac{1}{4}\\sum_j 6 y_j\\frac{\\delta y_j}{\\delta x_i} & \\\\\n",
    "&= \\frac{3}{2}\\sum_j (x_j + 2)\\frac{\\delta (x_j + 2)}{\\delta x_i} & \\\\\n",
    "&= \\frac{3}{2}\\sum_j (x_j + 2)\\frac{\\delta x_j}{\\delta x_i} & \\\\\n",
    "&= \\frac{3}{2}(x_i + 2) & \\frac{\\delta x_j}{\\delta x_i} = 0 \\;\\textrm{if}\\; i \\neq j \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Como $x$ es una matrix $2 \\times 2$, el gradiente va a ser una matriz de $2 \\times 2$.\n",
    "\n",
    "$$\\textrm{x.grad} = \\frac{\\delta f}{\\delta x_i}\\bigg\\rvert_{x_i=x} = \\begin{bmatrix}\\frac{3}{2}(2 + 2) & \\frac{3}{2}(1 + 2) \\\\ \\frac{3}{2}(0 + 2) & \\frac{3}{2}(1 + 2)\\end{bmatrix} = \\begin{bmatrix}6 & 4.5 \\\\ 3 & 4.5\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0000, 4.5000],\n",
      "        [3.0000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2,1],[0,1]], dtype=torch.float, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "f = z.mean()\n",
    "\n",
    "f.backward(torch.ones(1,1))\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de modelo no lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Airline dataset')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt('passenger_data.txt')\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "r = 0.8 #training vesus all ratio\n",
    "til_train = int(len(y)*r)\n",
    "#x = x-til_train\n",
    "\n",
    "z = 10 * np.sin(x*0.51) * np.exp(x*0.02) + 100 + x*2.49\n",
    "\n",
    "x_trn = x[:int(len(y)*r)]\n",
    "y_trn = y[:int(len(y)*r)]\n",
    "\n",
    "x_tst = x[int(len(y)*r):]\n",
    "y_tst = y[int(len(y)*r):]\n",
    "print(y.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(x_tst,y_tst, 'or')\n",
    "plt.plot(x_trn,y_trn, 'og')\n",
    "plt.title('Airline dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo_nolineal()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class modelo_nolineal(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(modelo_nolineal, self).__init__()\n",
    "        #self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        #self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        #self.c = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        #self.d = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        #self.e = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        #self.f = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.a = nn.Parameter(torch.tensor([100], dtype=torch.float, requires_grad=True))\n",
    "        self.b = nn.Parameter(torch.tensor([2], dtype=torch.float, requires_grad=True))\n",
    "        self.c = nn.Parameter(torch.tensor([10], dtype=torch.float, requires_grad=True))\n",
    "        self.d = nn.Parameter(torch.tensor([1], dtype=torch.float, requires_grad=True))\n",
    "        self.e = nn.Parameter(torch.tensor([0.01], dtype=torch.float, requires_grad=True))\n",
    "        self.f = nn.Parameter(torch.tensor([0], dtype=torch.float, requires_grad=True))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #return self.a + self.b * x\n",
    "        bias = self.a\n",
    "        slope = self.b\n",
    "        growth = self.c * torch.exp(self.d * x)\n",
    "        oscillatory = torch.cos(2*np.pi* x/12 + self.f)\n",
    "        return bias + slope * x + growth * oscillatory\n",
    "        #return self.a + self.b*x + self.c*x**2 \n",
    "\n",
    "\n",
    "modelo = modelo_nolineal()\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo_NN(\n",
      "  (fc1): Linear(in_features=1, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=240, bias=True)\n",
      "  (fc3): Linear(in_features=240, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=150, bias=True)\n",
      "  (fc5): Linear(in_features=150, out_features=100, bias=True)\n",
      "  (fc6): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc7): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class modelo_NN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(modelo_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 120)  \n",
    "        self.fc2 = nn.Linear(120, 240)\n",
    "        self.fc3 = nn.Linear(240, 200)\n",
    "        self.fc4 = nn.Linear(200, 150)\n",
    "        self.fc5 = nn.Linear(150, 100)\n",
    "        self.fc6 = nn.Linear(100, 50)\n",
    "        self.fc7 = nn.Linear(50, 1)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "modelo = modelo_NN()\n",
    "print(modelo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capas: 14\n",
      "Tamaño de parámetro 1 => torch.Size([120, 1])\n",
      "Tamaño de parámetro 2 => torch.Size([120])\n",
      "Tamaño de parámetro 3 => torch.Size([240, 120])\n",
      "Tamaño de parámetro 4 => torch.Size([240])\n",
      "Tamaño de parámetro 5 => torch.Size([200, 240])\n",
      "Tamaño de parámetro 6 => torch.Size([200])\n",
      "Tamaño de parámetro 7 => torch.Size([150, 200])\n",
      "Tamaño de parámetro 8 => torch.Size([150])\n",
      "Tamaño de parámetro 9 => torch.Size([100, 150])\n",
      "Tamaño de parámetro 10 => torch.Size([100])\n",
      "Tamaño de parámetro 11 => torch.Size([50, 100])\n",
      "Tamaño de parámetro 12 => torch.Size([50])\n",
      "Tamaño de parámetro 13 => torch.Size([1, 50])\n",
      "Tamaño de parámetro 14 => torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "params = list(modelo.parameters())\n",
    "print(\"Capas:\", len(params))\n",
    "for i, param in enumerate(params):\n",
    "    print(\"Tamaño de parámetro\", i+1, \"=>\", param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de perdida\n",
    "Una función de perdida toma una salida y el objetivo de su valor para calcular una medida de error. Por ejemplo, el `nn.MSELoss` que calcula el error de escuadrados mínimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0678], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1)\n",
    "out = modelo(input)\n",
    "print(out)  # tensor con tamaño 1x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1492, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = modelo(input)\n",
    "target = torch.randn(1)  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos propagación hacia atrás para empotrar los valores de error en nuestros cadas de red neuronal. En PyTorch hacemos el \"backprop\" simplemente usando `backward()` en la salida de función de perdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.zero_grad()  # borrar los valores de antes de todos parámetros\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "#modelo.a.grad\n",
    "#net.conv1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualizar los pesos\n",
    "El método más simple de actualizar los pesos es el \"Stochastic Gradient Descent (SGD)\". En PyTorch podemos usar el packete `torch.optim` que nos da implementaciones de varios tipos de reglas de actualizar (como SGD, Nesterov-SGD, Adam, RMSProp, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = modelo(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "\n",
    "tensor_x = torch.from_numpy(x_trn).float()\n",
    "tensor_y = torch.from_numpy(y_trn).float()\n",
    "\n",
    "dataset_tr = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "b_size = 10\n",
    "trainloader = utils.DataLoader(dataset_tr, batch_size=b_size) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    12] loss: 33253627951.100\n",
      "[2,    12] loss: 267822.195\n",
      "[3,    12] loss: 872664.761\n",
      "[4,    12] loss: 357030.815\n",
      "[5,    12] loss: 77191.034\n",
      "[6,    12] loss: 26747.622\n",
      "[7,    12] loss: 26271.587\n",
      "[8,    12] loss: 23825.047\n",
      "[9,    12] loss: 20686.058\n",
      "[10,    12] loss: 18389.493\n",
      "[11,    12] loss: 22268.879\n",
      "[12,    12] loss: 19525.963\n",
      "[13,    12] loss: 18100.708\n",
      "[14,    12] loss: 17049.612\n",
      "[15,    12] loss: 16820.573\n",
      "[16,    12] loss: 22542.012\n",
      "[17,    12] loss: 16796.504\n",
      "[18,    12] loss: 16690.619\n",
      "[19,    12] loss: 16777.618\n",
      "[20,    12] loss: 16490.461\n",
      "[21,    12] loss: 16103.151\n",
      "[22,    12] loss: 16265.027\n",
      "[23,    12] loss: 15914.524\n",
      "[24,    12] loss: 15900.752\n",
      "[25,    12] loss: 15810.957\n",
      "[26,    12] loss: 15804.400\n",
      "[27,    12] loss: 15805.474\n",
      "[28,    12] loss: 16333.206\n",
      "[29,    12] loss: 15665.586\n",
      "[30,    12] loss: 15595.918\n",
      "[31,    12] loss: 16336.602\n",
      "[32,    12] loss: 16238.112\n",
      "[33,    12] loss: 16863.115\n",
      "[34,    12] loss: 16627.365\n",
      "[35,    12] loss: 15984.428\n",
      "[36,    12] loss: 16687.937\n",
      "[37,    12] loss: 16084.286\n",
      "[38,    12] loss: 21669.731\n",
      "[39,    12] loss: 21312.716\n",
      "[40,    12] loss: 19917.044\n",
      "[41,    12] loss: 17719.437\n",
      "[42,    12] loss: 17206.532\n",
      "[43,    12] loss: 17409.511\n",
      "[44,    12] loss: 16939.955\n",
      "[45,    12] loss: 17245.675\n",
      "[46,    12] loss: 16787.861\n",
      "[47,    12] loss: 18503.177\n",
      "[48,    12] loss: 17211.661\n",
      "[49,    12] loss: 17736.340\n",
      "[50,    12] loss: 16916.631\n",
      "[51,    12] loss: 19318.779\n",
      "[52,    12] loss: 17923.154\n",
      "[53,    12] loss: 17000.507\n",
      "[54,    12] loss: 17412.099\n",
      "[55,    12] loss: 17096.918\n",
      "[56,    12] loss: 19494.430\n",
      "[57,    12] loss: 18135.805\n",
      "[58,    12] loss: 16180.114\n",
      "[59,    12] loss: 16092.991\n",
      "[60,    12] loss: 15451.404\n",
      "[61,    12] loss: 15194.757\n",
      "[62,    12] loss: 15249.746\n",
      "[63,    12] loss: 15302.205\n",
      "[64,    12] loss: 15247.127\n",
      "[65,    12] loss: 15227.534\n",
      "[66,    12] loss: 15352.480\n",
      "[67,    12] loss: 15279.510\n",
      "[68,    12] loss: 15252.570\n",
      "[69,    12] loss: 15248.646\n",
      "[70,    12] loss: 15271.671\n",
      "[71,    12] loss: 15324.424\n",
      "[72,    12] loss: 15387.758\n",
      "[73,    12] loss: 15236.677\n",
      "[74,    12] loss: 15312.462\n",
      "[75,    12] loss: 15293.758\n",
      "[76,    12] loss: 15245.822\n",
      "[77,    12] loss: 15402.794\n",
      "[78,    12] loss: 15187.580\n",
      "[79,    12] loss: 15446.510\n",
      "[80,    12] loss: 15146.662\n",
      "[81,    12] loss: 15535.445\n",
      "[82,    12] loss: 15173.293\n",
      "[83,    12] loss: 15463.942\n",
      "[84,    12] loss: 15362.365\n",
      "[85,    12] loss: 15328.700\n",
      "[86,    12] loss: 15290.300\n",
      "[87,    12] loss: 15314.394\n",
      "[88,    12] loss: 15250.405\n",
      "[89,    12] loss: 15417.974\n",
      "[90,    12] loss: 15183.775\n",
      "[91,    12] loss: 15536.875\n",
      "[92,    12] loss: 15109.971\n",
      "[93,    12] loss: 15495.254\n",
      "[94,    12] loss: 15021.540\n",
      "[95,    12] loss: 15539.602\n",
      "[96,    12] loss: 15068.092\n",
      "[97,    12] loss: 15546.129\n",
      "[98,    12] loss: 15248.569\n",
      "[99,    12] loss: 15458.792\n",
      "[100,    12] loss: 15282.549\n",
      "[101,    12] loss: 15447.065\n",
      "[102,    12] loss: 15333.152\n",
      "[103,    12] loss: 15444.445\n",
      "[104,    12] loss: 15358.071\n",
      "[105,    12] loss: 15385.097\n",
      "[106,    12] loss: 15393.082\n",
      "[107,    12] loss: 15367.969\n",
      "[108,    12] loss: 15407.387\n",
      "[109,    12] loss: 15352.836\n",
      "[110,    12] loss: 15384.342\n",
      "[111,    12] loss: 15478.313\n",
      "[112,    12] loss: 15359.109\n",
      "[113,    12] loss: 15424.131\n",
      "[114,    12] loss: 15395.776\n",
      "[115,    12] loss: 15351.276\n",
      "[116,    12] loss: 15429.947\n",
      "[117,    12] loss: 15374.789\n",
      "[118,    12] loss: 15385.510\n",
      "[119,    12] loss: 15379.147\n",
      "[120,    12] loss: 15290.412\n",
      "[121,    12] loss: 15278.251\n",
      "[122,    12] loss: 15375.054\n",
      "[123,    12] loss: 15285.492\n",
      "[124,    12] loss: 15298.218\n",
      "[125,    12] loss: 15344.758\n",
      "[126,    12] loss: 15342.589\n",
      "[127,    12] loss: 15373.436\n",
      "[128,    12] loss: 15297.378\n",
      "[129,    12] loss: 15281.228\n",
      "[130,    12] loss: 15343.705\n",
      "[131,    12] loss: 15332.956\n",
      "[132,    12] loss: 15317.705\n",
      "[133,    12] loss: 15262.403\n",
      "[134,    12] loss: 15297.355\n",
      "[135,    12] loss: 15318.103\n",
      "[136,    12] loss: 15295.173\n",
      "[137,    12] loss: 15266.650\n",
      "[138,    12] loss: 15312.321\n",
      "[139,    12] loss: 15263.908\n",
      "[140,    12] loss: 15282.548\n",
      "[141,    12] loss: 15291.366\n",
      "[142,    12] loss: 15280.546\n",
      "[143,    12] loss: 15279.391\n",
      "[144,    12] loss: 15274.261\n",
      "[145,    12] loss: 15284.429\n",
      "[146,    12] loss: 15280.424\n",
      "[147,    12] loss: 15231.396\n",
      "[148,    12] loss: 15321.701\n",
      "[149,    12] loss: 15298.443\n",
      "[150,    12] loss: 15310.493\n",
      "[151,    12] loss: 15280.781\n",
      "[152,    12] loss: 15271.045\n",
      "[153,    12] loss: 15277.635\n",
      "[154,    12] loss: 15351.281\n",
      "[155,    12] loss: 15269.828\n",
      "[156,    12] loss: 15272.881\n",
      "[157,    12] loss: 15269.100\n",
      "[158,    12] loss: 15299.693\n",
      "[159,    12] loss: 15266.615\n",
      "[160,    12] loss: 15326.580\n",
      "[161,    12] loss: 15263.000\n",
      "[162,    12] loss: 15315.919\n",
      "[163,    12] loss: 15267.861\n",
      "[164,    12] loss: 15297.364\n",
      "[165,    12] loss: 15276.479\n",
      "[166,    12] loss: 15297.077\n",
      "[167,    12] loss: 15269.728\n",
      "[168,    12] loss: 15315.778\n",
      "[169,    12] loss: 15262.012\n",
      "[170,    12] loss: 15270.215\n",
      "[171,    12] loss: 15295.121\n",
      "[172,    12] loss: 15274.465\n",
      "[173,    12] loss: 15280.655\n",
      "[174,    12] loss: 15276.290\n",
      "[175,    12] loss: 15251.252\n",
      "[176,    12] loss: 15257.630\n",
      "[177,    12] loss: 15228.953\n",
      "[178,    12] loss: 15239.610\n",
      "[179,    12] loss: 15245.517\n",
      "[180,    12] loss: 15202.261\n",
      "[181,    12] loss: 15266.398\n",
      "[182,    12] loss: 15231.846\n",
      "[183,    12] loss: 15304.882\n",
      "[184,    12] loss: 15217.740\n",
      "[185,    12] loss: 15291.375\n",
      "[186,    12] loss: 15234.768\n",
      "[187,    12] loss: 15259.163\n",
      "[188,    12] loss: 15246.359\n",
      "[189,    12] loss: 15244.336\n",
      "[190,    12] loss: 15226.147\n",
      "[191,    12] loss: 15255.442\n",
      "[192,    12] loss: 15202.843\n",
      "[193,    12] loss: 15314.565\n",
      "[194,    12] loss: 15220.136\n",
      "[195,    12] loss: 15295.958\n",
      "[196,    12] loss: 15259.728\n",
      "[197,    12] loss: 15368.195\n",
      "[198,    12] loss: 15215.371\n",
      "[199,    12] loss: 15350.148\n",
      "[200,    12] loss: 15580.078\n",
      "[201,    12] loss: 15616.696\n",
      "[202,    12] loss: 15414.412\n",
      "[203,    12] loss: 15780.958\n",
      "[204,    12] loss: 15238.696\n",
      "[205,    12] loss: 15748.480\n",
      "[206,    12] loss: 15359.165\n",
      "[207,    12] loss: 15427.464\n",
      "[208,    12] loss: 15909.867\n",
      "[209,    12] loss: 15295.505\n",
      "[210,    12] loss: 15778.648\n",
      "[211,    12] loss: 15211.855\n",
      "[212,    12] loss: 15579.850\n",
      "[213,    12] loss: 15143.996\n",
      "[214,    12] loss: 15768.818\n",
      "[215,    12] loss: 15183.169\n",
      "[216,    12] loss: 15639.667\n",
      "[217,    12] loss: 15213.233\n",
      "[218,    12] loss: 15457.471\n",
      "[219,    12] loss: 15213.364\n",
      "[220,    12] loss: 15379.094\n",
      "[221,    12] loss: 15266.646\n",
      "[222,    12] loss: 15490.803\n",
      "[223,    12] loss: 15276.262\n",
      "[224,    12] loss: 15869.189\n",
      "[225,    12] loss: 15195.977\n",
      "[226,    12] loss: 15928.007\n",
      "[227,    12] loss: 15151.661\n",
      "[228,    12] loss: 16223.756\n",
      "[229,    12] loss: 15266.098\n",
      "[230,    12] loss: 15852.563\n",
      "[231,    12] loss: 15269.119\n",
      "[232,    12] loss: 15997.092\n",
      "[233,    12] loss: 15201.153\n",
      "[234,    12] loss: 15832.759\n",
      "[235,    12] loss: 15173.675\n",
      "[236,    12] loss: 15684.825\n",
      "[237,    12] loss: 15265.613\n",
      "[238,    12] loss: 15960.754\n",
      "[239,    12] loss: 15200.328\n",
      "[240,    12] loss: 16624.457\n",
      "[241,    12] loss: 15498.752\n",
      "[242,    12] loss: 15572.415\n",
      "[243,    12] loss: 15840.827\n",
      "[244,    12] loss: 15367.722\n",
      "[245,    12] loss: 15948.497\n",
      "[246,    12] loss: 15344.866\n",
      "[247,    12] loss: 16978.235\n",
      "[248,    12] loss: 15834.125\n",
      "[249,    12] loss: 15573.045\n",
      "[250,    12] loss: 16199.286\n",
      "[251,    12] loss: 15419.894\n",
      "[252,    12] loss: 15565.722\n",
      "[253,    12] loss: 15978.358\n",
      "[254,    12] loss: 15327.814\n",
      "[255,    12] loss: 16124.311\n",
      "[256,    12] loss: 15422.979\n",
      "[257,    12] loss: 16065.154\n",
      "[258,    12] loss: 15481.756\n",
      "[259,    12] loss: 15785.220\n",
      "[260,    12] loss: 15618.674\n",
      "[261,    12] loss: 15738.987\n",
      "[262,    12] loss: 15611.325\n",
      "[263,    12] loss: 16367.731\n",
      "[264,    12] loss: 15593.396\n",
      "[265,    12] loss: 16037.602\n",
      "[266,    12] loss: 15483.512\n",
      "[267,    12] loss: 16293.023\n",
      "[268,    12] loss: 15561.436\n",
      "[269,    12] loss: 16032.842\n",
      "[270,    12] loss: 15585.828\n",
      "[271,    12] loss: 16686.874\n",
      "[272,    12] loss: 15777.443\n",
      "[273,    12] loss: 16228.106\n",
      "[274,    12] loss: 15659.804\n",
      "[275,    12] loss: 16597.520\n",
      "[276,    12] loss: 15729.942\n",
      "[277,    12] loss: 16372.807\n",
      "[278,    12] loss: 15677.976\n",
      "[279,    12] loss: 17068.012\n",
      "[280,    12] loss: 15724.939\n",
      "[281,    12] loss: 16641.491\n",
      "[282,    12] loss: 15815.502\n",
      "[283,    12] loss: 16384.012\n",
      "[284,    12] loss: 15766.180\n",
      "[285,    12] loss: 16224.267\n",
      "[286,    12] loss: 15823.827\n",
      "[287,    12] loss: 16248.245\n",
      "[288,    12] loss: 15883.423\n",
      "[289,    12] loss: 16348.726\n",
      "[290,    12] loss: 15892.166\n",
      "[291,    12] loss: 16269.641\n",
      "[292,    12] loss: 15879.370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293,    12] loss: 16238.258\n",
      "[294,    12] loss: 16095.742\n",
      "[295,    12] loss: 16067.693\n",
      "[296,    12] loss: 16206.870\n",
      "[297,    12] loss: 16095.727\n",
      "[298,    12] loss: 16061.632\n",
      "[299,    12] loss: 16280.380\n",
      "[300,    12] loss: 16033.868\n",
      "[301,    12] loss: 16179.651\n",
      "[302,    12] loss: 15944.266\n",
      "[303,    12] loss: 16134.808\n",
      "[304,    12] loss: 15918.476\n",
      "[305,    12] loss: 16043.813\n",
      "[306,    12] loss: 15821.045\n",
      "[307,    12] loss: 15941.183\n",
      "[308,    12] loss: 15776.721\n",
      "[309,    12] loss: 15728.615\n",
      "[310,    12] loss: 15752.457\n",
      "[311,    12] loss: 15741.598\n",
      "[312,    12] loss: 15725.522\n",
      "[313,    12] loss: 15702.988\n",
      "[314,    12] loss: 15683.775\n",
      "[315,    12] loss: 15659.438\n",
      "[316,    12] loss: 15642.469\n",
      "[317,    12] loss: 15633.419\n",
      "[318,    12] loss: 15649.005\n",
      "[319,    12] loss: 15593.002\n",
      "[320,    12] loss: 15638.731\n",
      "[321,    12] loss: 15590.081\n",
      "[322,    12] loss: 15619.240\n",
      "[323,    12] loss: 15535.062\n",
      "[324,    12] loss: 15586.397\n",
      "[325,    12] loss: 15493.250\n",
      "[326,    12] loss: 15569.486\n",
      "[327,    12] loss: 15471.047\n",
      "[328,    12] loss: 15526.418\n",
      "[329,    12] loss: 15451.708\n",
      "[330,    12] loss: 15493.149\n",
      "[331,    12] loss: 15421.608\n",
      "[332,    12] loss: 15447.262\n",
      "[333,    12] loss: 15405.274\n",
      "[334,    12] loss: 15437.471\n",
      "[335,    12] loss: 15387.999\n",
      "[336,    12] loss: 15424.792\n",
      "[337,    12] loss: 15386.058\n",
      "[338,    12] loss: 15392.845\n",
      "[339,    12] loss: 15381.434\n",
      "[340,    12] loss: 15365.497\n",
      "[341,    12] loss: 15347.700\n",
      "[342,    12] loss: 15339.036\n",
      "[343,    12] loss: 15353.764\n",
      "[344,    12] loss: 15334.450\n",
      "[345,    12] loss: 15329.571\n",
      "[346,    12] loss: 15346.887\n",
      "[347,    12] loss: 15305.890\n",
      "[348,    12] loss: 15339.771\n",
      "[349,    12] loss: 15320.469\n",
      "[350,    12] loss: 15309.776\n",
      "[351,    12] loss: 15298.111\n",
      "[352,    12] loss: 15305.549\n",
      "[353,    12] loss: 15274.461\n",
      "[354,    12] loss: 15300.802\n",
      "[355,    12] loss: 15280.391\n",
      "[356,    12] loss: 15293.863\n",
      "[357,    12] loss: 15289.505\n",
      "[358,    12] loss: 15266.479\n",
      "[359,    12] loss: 15272.573\n",
      "[360,    12] loss: 15240.929\n",
      "[361,    12] loss: 15272.004\n",
      "[362,    12] loss: 15232.161\n",
      "[363,    12] loss: 15271.080\n",
      "[364,    12] loss: 15225.536\n",
      "[365,    12] loss: 15271.365\n",
      "[366,    12] loss: 15240.806\n",
      "[367,    12] loss: 15252.246\n",
      "[368,    12] loss: 15205.058\n",
      "[369,    12] loss: 15257.576\n",
      "[370,    12] loss: 15233.761\n",
      "[371,    12] loss: 15232.930\n",
      "[372,    12] loss: 15202.112\n",
      "[373,    12] loss: 15247.397\n",
      "[374,    12] loss: 15198.562\n",
      "[375,    12] loss: 15252.377\n",
      "[376,    12] loss: 15196.995\n",
      "[377,    12] loss: 15252.018\n",
      "[378,    12] loss: 15231.766\n",
      "[379,    12] loss: 15182.762\n",
      "[380,    12] loss: 15242.416\n",
      "[381,    12] loss: 15208.873\n",
      "[382,    12] loss: 15235.187\n",
      "[383,    12] loss: 15180.247\n",
      "[384,    12] loss: 15236.775\n",
      "[385,    12] loss: 15218.473\n",
      "[386,    12] loss: 15207.426\n",
      "[387,    12] loss: 15191.983\n",
      "[388,    12] loss: 15219.100\n",
      "[389,    12] loss: 15204.877\n",
      "[390,    12] loss: 15215.432\n",
      "[391,    12] loss: 15199.832\n",
      "[392,    12] loss: 15215.522\n",
      "[393,    12] loss: 15195.876\n",
      "[394,    12] loss: 15218.554\n",
      "[395,    12] loss: 15198.064\n",
      "[396,    12] loss: 15212.709\n",
      "[397,    12] loss: 15191.369\n",
      "[398,    12] loss: 15204.297\n",
      "[399,    12] loss: 15196.597\n",
      "[400,    12] loss: 15200.665\n",
      "[401,    12] loss: 15194.700\n",
      "[402,    12] loss: 15194.974\n",
      "[403,    12] loss: 15193.601\n",
      "[404,    12] loss: 15196.311\n",
      "[405,    12] loss: 15191.387\n",
      "[406,    12] loss: 15191.124\n",
      "[407,    12] loss: 15196.271\n",
      "[408,    12] loss: 15188.158\n",
      "[409,    12] loss: 15166.942\n",
      "[410,    12] loss: 15194.723\n",
      "[411,    12] loss: 15179.196\n",
      "[412,    12] loss: 15177.473\n",
      "[413,    12] loss: 15187.721\n",
      "[414,    12] loss: 15182.213\n",
      "[415,    12] loss: 15183.614\n",
      "[416,    12] loss: 15182.838\n",
      "[417,    12] loss: 15177.137\n",
      "[418,    12] loss: 15169.999\n",
      "[419,    12] loss: 15175.434\n",
      "[420,    12] loss: 15172.266\n",
      "[421,    12] loss: 15173.829\n",
      "[422,    12] loss: 15160.904\n",
      "[423,    12] loss: 15173.237\n",
      "[424,    12] loss: 15165.665\n",
      "[425,    12] loss: 15167.219\n",
      "[426,    12] loss: 15169.464\n",
      "[427,    12] loss: 15170.381\n",
      "[428,    12] loss: 15156.826\n",
      "[429,    12] loss: 15169.811\n",
      "[430,    12] loss: 15164.898\n",
      "[431,    12] loss: 15155.897\n",
      "[432,    12] loss: 15136.433\n",
      "[433,    12] loss: 15161.152\n",
      "[434,    12] loss: 15154.995\n",
      "[435,    12] loss: 15146.731\n",
      "[436,    12] loss: 15146.101\n",
      "[437,    12] loss: 15149.937\n",
      "[438,    12] loss: 15129.172\n",
      "[439,    12] loss: 15152.446\n",
      "[440,    12] loss: 15154.154\n",
      "[441,    12] loss: 15144.933\n",
      "[442,    12] loss: 15149.332\n",
      "[443,    12] loss: 15142.133\n",
      "[444,    12] loss: 15125.042\n",
      "[445,    12] loss: 15154.286\n",
      "[446,    12] loss: 15131.357\n",
      "[447,    12] loss: 15133.466\n",
      "[448,    12] loss: 15135.862\n",
      "[449,    12] loss: 15157.587\n",
      "[450,    12] loss: 15127.604\n",
      "[451,    12] loss: 15112.694\n",
      "[452,    12] loss: 15136.543\n",
      "[453,    12] loss: 15145.888\n",
      "[454,    12] loss: 15124.306\n",
      "[455,    12] loss: 15144.936\n",
      "[456,    12] loss: 15118.305\n",
      "[457,    12] loss: 15139.569\n",
      "[458,    12] loss: 15136.425\n",
      "[459,    12] loss: 15125.700\n",
      "[460,    12] loss: 15119.644\n",
      "[461,    12] loss: 15120.614\n",
      "[462,    12] loss: 15108.500\n",
      "[463,    12] loss: 15118.362\n",
      "[464,    12] loss: 15122.353\n",
      "[465,    12] loss: 15115.407\n",
      "[466,    12] loss: 15106.156\n",
      "[467,    12] loss: 15152.385\n",
      "[468,    12] loss: 15114.180\n",
      "[469,    12] loss: 15091.437\n",
      "[470,    12] loss: 15118.100\n",
      "[471,    12] loss: 15144.916\n",
      "[472,    12] loss: 15077.378\n",
      "[473,    12] loss: 15128.504\n",
      "[474,    12] loss: 15068.334\n",
      "[475,    12] loss: 15121.906\n",
      "[476,    12] loss: 15071.868\n",
      "[477,    12] loss: 15112.480\n",
      "[478,    12] loss: 15087.079\n",
      "[479,    12] loss: 15058.282\n",
      "[480,    12] loss: 15126.889\n",
      "[481,    12] loss: 15079.335\n",
      "[482,    12] loss: 15131.144\n",
      "[483,    12] loss: 15197.727\n",
      "[484,    12] loss: 15063.756\n",
      "[485,    12] loss: 15120.270\n",
      "[486,    12] loss: 15067.450\n",
      "[487,    12] loss: 15122.496\n",
      "[488,    12] loss: 15070.920\n",
      "[489,    12] loss: 15084.948\n",
      "[490,    12] loss: 15095.662\n",
      "[491,    12] loss: 15082.006\n",
      "[492,    12] loss: 15081.989\n",
      "[493,    12] loss: 15079.300\n",
      "[494,    12] loss: 15071.741\n",
      "[495,    12] loss: 15080.167\n",
      "[496,    12] loss: 15077.814\n",
      "[497,    12] loss: 15050.615\n",
      "[498,    12] loss: 15088.406\n",
      "[499,    12] loss: 15063.063\n",
      "[500,    12] loss: 15080.939\n",
      "[501,    12] loss: 15084.636\n",
      "[502,    12] loss: 15039.520\n",
      "[503,    12] loss: 15080.794\n",
      "[504,    12] loss: 15060.680\n",
      "[505,    12] loss: 15079.253\n",
      "[506,    12] loss: 15081.983\n",
      "[507,    12] loss: 15037.255\n",
      "[508,    12] loss: 15079.176\n",
      "[509,    12] loss: 15056.687\n",
      "[510,    12] loss: 15072.459\n",
      "[511,    12] loss: 15074.252\n",
      "[512,    12] loss: 15037.803\n",
      "[513,    12] loss: 15074.415\n",
      "[514,    12] loss: 15059.730\n",
      "[515,    12] loss: 15072.027\n",
      "[516,    12] loss: 15084.028\n",
      "[517,    12] loss: 15032.501\n",
      "[518,    12] loss: 15058.810\n",
      "[519,    12] loss: 15080.035\n",
      "[520,    12] loss: 15068.353\n",
      "[521,    12] loss: 15044.550\n",
      "[522,    12] loss: 15071.102\n",
      "[523,    12] loss: 15067.123\n",
      "[524,    12] loss: 15022.179\n",
      "[525,    12] loss: 15068.288\n",
      "[526,    12] loss: 15076.416\n",
      "[527,    12] loss: 15025.915\n",
      "[528,    12] loss: 15053.957\n",
      "[529,    12] loss: 15093.470\n",
      "[530,    12] loss: 15059.160\n",
      "[531,    12] loss: 15024.657\n",
      "[532,    12] loss: 15049.271\n",
      "[533,    12] loss: 15023.419\n",
      "[534,    12] loss: 15054.595\n",
      "[535,    12] loss: 15052.515\n",
      "[536,    12] loss: 15046.061\n",
      "[537,    12] loss: 15049.622\n",
      "[538,    12] loss: 15008.796\n",
      "[539,    12] loss: 15056.153\n",
      "[540,    12] loss: 15063.267\n",
      "[541,    12] loss: 15105.921\n",
      "[542,    12] loss: 15150.084\n",
      "[543,    12] loss: 15025.919\n",
      "[544,    12] loss: 15041.523\n",
      "[545,    12] loss: 15078.075\n",
      "[546,    12] loss: 15083.121\n",
      "[547,    12] loss: 15053.656\n",
      "[548,    12] loss: 15202.316\n",
      "[549,    12] loss: 15074.099\n",
      "[550,    12] loss: 15004.367\n",
      "[551,    12] loss: 15045.936\n",
      "[552,    12] loss: 15033.610\n",
      "[553,    12] loss: 15026.232\n",
      "[554,    12] loss: 15237.122\n",
      "[555,    12] loss: 15058.627\n",
      "[556,    12] loss: 15019.101\n",
      "[557,    12] loss: 15049.921\n",
      "[558,    12] loss: 15020.752\n",
      "[559,    12] loss: 15057.287\n",
      "[560,    12] loss: 15118.773\n",
      "[561,    12] loss: 15033.485\n",
      "[562,    12] loss: 15190.392\n",
      "[563,    12] loss: 15045.432\n",
      "[564,    12] loss: 15016.969\n",
      "[565,    12] loss: 15032.733\n",
      "[566,    12] loss: 15018.558\n",
      "[567,    12] loss: 15042.855\n",
      "[568,    12] loss: 15088.987\n",
      "[569,    12] loss: 15024.950\n",
      "[570,    12] loss: 15196.726\n",
      "[571,    12] loss: 15058.059\n",
      "[572,    12] loss: 15011.608\n",
      "[573,    12] loss: 15006.329\n",
      "[574,    12] loss: 15089.882\n",
      "[575,    12] loss: 15009.986\n",
      "[576,    12] loss: 15049.356\n",
      "[577,    12] loss: 15030.801\n",
      "[578,    12] loss: 15016.621\n",
      "[579,    12] loss: 14995.749\n",
      "[580,    12] loss: 15088.444\n",
      "[581,    12] loss: 15035.248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[582,    12] loss: 15007.380\n",
      "[583,    12] loss: 15041.127\n",
      "[584,    12] loss: 15086.347\n",
      "[585,    12] loss: 15025.673\n",
      "[586,    12] loss: 15108.372\n",
      "[587,    12] loss: 15008.388\n",
      "[588,    12] loss: 15098.027\n",
      "[589,    12] loss: 15045.571\n",
      "[590,    12] loss: 15038.688\n",
      "[591,    12] loss: 15020.997\n",
      "[592,    12] loss: 15162.163\n",
      "[593,    12] loss: 15032.488\n",
      "[594,    12] loss: 15003.575\n",
      "[595,    12] loss: 14992.101\n",
      "[596,    12] loss: 15026.266\n",
      "[597,    12] loss: 15027.295\n",
      "[598,    12] loss: 15033.710\n",
      "[599,    12] loss: 14985.642\n",
      "[600,    12] loss: 15097.991\n",
      "[601,    12] loss: 15041.502\n",
      "[602,    12] loss: 14999.076\n",
      "[603,    12] loss: 15002.361\n",
      "[604,    12] loss: 15050.705\n",
      "[605,    12] loss: 15021.174\n",
      "[606,    12] loss: 14982.916\n",
      "[607,    12] loss: 14974.255\n",
      "[608,    12] loss: 15048.450\n",
      "[609,    12] loss: 14979.608\n",
      "[610,    12] loss: 14984.081\n",
      "[611,    12] loss: 15042.698\n",
      "[612,    12] loss: 14979.568\n",
      "[613,    12] loss: 15008.112\n",
      "[614,    12] loss: 14990.373\n",
      "[615,    12] loss: 14982.389\n",
      "[616,    12] loss: 15009.767\n",
      "[617,    12] loss: 14975.495\n",
      "[618,    12] loss: 15006.345\n",
      "[619,    12] loss: 14998.999\n",
      "[620,    12] loss: 14964.346\n",
      "[621,    12] loss: 15029.647\n",
      "[622,    12] loss: 15007.418\n",
      "[623,    12] loss: 15022.904\n",
      "[624,    12] loss: 14988.915\n",
      "[625,    12] loss: 14962.107\n",
      "[626,    12] loss: 15030.411\n",
      "[627,    12] loss: 14972.305\n",
      "[628,    12] loss: 15016.261\n",
      "[629,    12] loss: 15034.729\n",
      "[630,    12] loss: 14980.492\n",
      "[631,    12] loss: 14983.617\n",
      "[632,    12] loss: 15004.684\n",
      "[633,    12] loss: 14998.276\n",
      "[634,    12] loss: 14981.767\n",
      "[635,    12] loss: 14970.300\n",
      "[636,    12] loss: 15019.878\n",
      "[637,    12] loss: 14979.734\n",
      "[638,    12] loss: 15018.762\n",
      "[639,    12] loss: 14972.214\n",
      "[640,    12] loss: 14958.919\n",
      "[641,    12] loss: 15020.355\n",
      "[642,    12] loss: 15007.032\n",
      "[643,    12] loss: 15030.610\n",
      "[644,    12] loss: 15051.596\n",
      "[645,    12] loss: 14960.583\n",
      "[646,    12] loss: 15023.035\n",
      "[647,    12] loss: 14989.126\n",
      "[648,    12] loss: 14953.069\n",
      "[649,    12] loss: 15267.387\n",
      "[650,    12] loss: 15016.297\n",
      "[651,    12] loss: 14968.567\n",
      "[652,    12] loss: 14978.485\n",
      "[653,    12] loss: 14994.549\n",
      "[654,    12] loss: 14957.230\n",
      "[655,    12] loss: 14977.717\n",
      "[656,    12] loss: 14995.877\n",
      "[657,    12] loss: 14949.286\n",
      "[658,    12] loss: 14995.822\n",
      "[659,    12] loss: 14962.867\n",
      "[660,    12] loss: 14949.826\n",
      "[661,    12] loss: 15008.908\n",
      "[662,    12] loss: 15000.019\n",
      "[663,    12] loss: 14998.691\n",
      "[664,    12] loss: 15041.169\n",
      "[665,    12] loss: 14952.089\n",
      "[666,    12] loss: 14964.587\n",
      "[667,    12] loss: 15044.757\n",
      "[668,    12] loss: 15018.696\n",
      "[669,    12] loss: 15006.834\n",
      "[670,    12] loss: 14952.123\n",
      "[671,    12] loss: 14984.399\n",
      "[672,    12] loss: 14951.680\n",
      "[673,    12] loss: 14958.142\n",
      "[674,    12] loss: 15032.715\n",
      "[675,    12] loss: 14977.176\n",
      "[676,    12] loss: 14996.331\n",
      "[677,    12] loss: 15148.186\n",
      "[678,    12] loss: 14997.237\n",
      "[679,    12] loss: 14979.006\n",
      "[680,    12] loss: 14991.499\n",
      "[681,    12] loss: 14955.867\n",
      "[682,    12] loss: 14939.826\n",
      "[683,    12] loss: 15002.018\n",
      "[684,    12] loss: 14969.897\n",
      "[685,    12] loss: 14953.015\n",
      "[686,    12] loss: 14936.977\n",
      "[687,    12] loss: 14994.098\n",
      "[688,    12] loss: 14995.136\n",
      "[689,    12] loss: 14980.231\n",
      "[690,    12] loss: 14944.363\n",
      "[691,    12] loss: 14948.843\n",
      "[692,    12] loss: 14997.034\n",
      "[693,    12] loss: 14934.241\n",
      "[694,    12] loss: 14935.806\n",
      "[695,    12] loss: 14994.134\n",
      "[696,    12] loss: 14993.655\n",
      "[697,    12] loss: 14979.648\n",
      "[698,    12] loss: 15017.830\n",
      "[699,    12] loss: 14954.226\n",
      "[700,    12] loss: 14928.893\n",
      "[701,    12] loss: 15009.903\n",
      "[702,    12] loss: 14982.659\n",
      "[703,    12] loss: 14957.543\n",
      "[704,    12] loss: 14953.281\n",
      "[705,    12] loss: 14950.852\n",
      "[706,    12] loss: 14961.170\n",
      "[707,    12] loss: 14931.081\n",
      "[708,    12] loss: 14980.996\n",
      "[709,    12] loss: 14944.594\n",
      "[710,    12] loss: 14962.987\n",
      "[711,    12] loss: 14992.095\n",
      "[712,    12] loss: 14927.838\n",
      "[713,    12] loss: 14937.196\n",
      "[714,    12] loss: 14979.662\n",
      "[715,    12] loss: 14927.993\n",
      "[716,    12] loss: 14924.935\n",
      "[717,    12] loss: 14984.764\n",
      "[718,    12] loss: 14970.106\n",
      "[719,    12] loss: 14967.029\n",
      "[720,    12] loss: 15029.823\n",
      "[721,    12] loss: 14956.140\n",
      "[722,    12] loss: 14963.363\n",
      "[723,    12] loss: 14971.143\n",
      "[724,    12] loss: 14961.346\n",
      "[725,    12] loss: 14926.032\n",
      "[726,    12] loss: 14963.640\n",
      "[727,    12] loss: 14936.953\n",
      "[728,    12] loss: 14921.411\n",
      "[729,    12] loss: 14971.408\n",
      "[730,    12] loss: 14946.818\n",
      "[731,    12] loss: 14973.901\n",
      "[732,    12] loss: 14939.123\n",
      "[733,    12] loss: 14945.616\n",
      "[734,    12] loss: 14965.039\n",
      "[735,    12] loss: 14924.026\n",
      "[736,    12] loss: 14915.865\n",
      "[737,    12] loss: 14960.113\n",
      "[738,    12] loss: 14927.415\n",
      "[739,    12] loss: 14915.857\n",
      "[740,    12] loss: 15017.638\n",
      "[741,    12] loss: 14955.267\n",
      "[742,    12] loss: 14970.535\n",
      "[743,    12] loss: 15096.430\n",
      "[744,    12] loss: 14965.624\n",
      "[745,    12] loss: 14991.968\n",
      "[746,    12] loss: 15021.022\n",
      "[747,    12] loss: 14946.147\n",
      "[748,    12] loss: 14954.831\n",
      "[749,    12] loss: 14950.842\n",
      "[750,    12] loss: 14949.272\n",
      "[751,    12] loss: 14921.319\n",
      "[752,    12] loss: 14947.630\n",
      "[753,    12] loss: 14927.616\n",
      "[754,    12] loss: 14920.515\n",
      "[755,    12] loss: 14918.669\n",
      "[756,    12] loss: 14957.622\n",
      "[757,    12] loss: 14912.443\n",
      "[758,    12] loss: 14910.271\n",
      "[759,    12] loss: 14959.864\n",
      "[760,    12] loss: 14971.361\n",
      "[761,    12] loss: 14946.223\n",
      "[762,    12] loss: 14948.674\n",
      "[763,    12] loss: 14932.470\n",
      "[764,    12] loss: 14916.750\n",
      "[765,    12] loss: 14926.806\n",
      "[766,    12] loss: 14927.754\n",
      "[767,    12] loss: 14901.167\n",
      "[768,    12] loss: 14942.214\n",
      "[769,    12] loss: 14914.000\n",
      "[770,    12] loss: 14894.486\n",
      "[771,    12] loss: 14984.890\n",
      "[772,    12] loss: 14952.647\n",
      "[773,    12] loss: 14938.553\n",
      "[774,    12] loss: 14949.316\n",
      "[775,    12] loss: 14944.929\n",
      "[776,    12] loss: 14894.574\n",
      "[777,    12] loss: 14906.440\n",
      "[778,    12] loss: 14956.155\n",
      "[779,    12] loss: 14926.016\n",
      "[780,    12] loss: 14966.158\n",
      "[781,    12] loss: 15054.375\n",
      "[782,    12] loss: 14933.224\n",
      "[783,    12] loss: 14929.966\n",
      "[784,    12] loss: 14935.119\n",
      "[785,    12] loss: 14943.253\n",
      "[786,    12] loss: 14894.499\n",
      "[787,    12] loss: 14913.360\n",
      "[788,    12] loss: 14940.688\n",
      "[789,    12] loss: 14898.537\n",
      "[790,    12] loss: 14926.505\n",
      "[791,    12] loss: 14938.640\n",
      "[792,    12] loss: 14900.015\n",
      "[793,    12] loss: 14894.702\n",
      "[794,    12] loss: 14930.202\n",
      "[795,    12] loss: 14919.577\n",
      "[796,    12] loss: 14949.069\n",
      "[797,    12] loss: 14901.438\n",
      "[798,    12] loss: 14910.182\n",
      "[799,    12] loss: 14926.204\n",
      "[800,    12] loss: 14942.865\n",
      "[801,    12] loss: 14925.394\n",
      "[802,    12] loss: 14893.743\n",
      "[803,    12] loss: 15033.441\n",
      "[804,    12] loss: 14904.348\n",
      "[805,    12] loss: 15040.291\n",
      "[806,    12] loss: 14949.593\n",
      "[807,    12] loss: 14893.943\n",
      "[808,    12] loss: 14920.661\n",
      "[809,    12] loss: 14945.864\n",
      "[810,    12] loss: 14933.045\n",
      "[811,    12] loss: 14917.797\n",
      "[812,    12] loss: 14935.881\n",
      "[813,    12] loss: 14904.537\n",
      "[814,    12] loss: 14902.831\n",
      "[815,    12] loss: 14911.201\n",
      "[816,    12] loss: 14953.804\n",
      "[817,    12] loss: 14945.214\n",
      "[818,    12] loss: 14912.994\n",
      "[819,    12] loss: 14928.254\n",
      "[820,    12] loss: 14943.160\n",
      "[821,    12] loss: 14883.177\n",
      "[822,    12] loss: 14889.718\n",
      "[823,    12] loss: 14957.187\n",
      "[824,    12] loss: 14894.713\n",
      "[825,    12] loss: 14925.069\n",
      "[826,    12] loss: 14916.320\n",
      "[827,    12] loss: 14894.098\n",
      "[828,    12] loss: 14923.230\n",
      "[829,    12] loss: 14888.550\n",
      "[830,    12] loss: 14883.161\n",
      "[831,    12] loss: 14959.103\n",
      "[832,    12] loss: 14918.194\n",
      "[833,    12] loss: 14915.019\n",
      "[834,    12] loss: 14922.135\n",
      "[835,    12] loss: 14926.931\n",
      "[836,    12] loss: 14887.392\n",
      "[837,    12] loss: 14890.161\n",
      "[838,    12] loss: 14910.964\n",
      "[839,    12] loss: 14886.261\n",
      "[840,    12] loss: 14879.112\n",
      "[841,    12] loss: 14975.079\n",
      "[842,    12] loss: 14919.055\n",
      "[843,    12] loss: 14923.764\n",
      "[844,    12] loss: 14927.274\n",
      "[845,    12] loss: 14898.544\n",
      "[846,    12] loss: 14884.320\n",
      "[847,    12] loss: 14938.732\n",
      "[848,    12] loss: 14913.654\n",
      "[849,    12] loss: 14902.740\n",
      "[850,    12] loss: 14913.353\n",
      "[851,    12] loss: 14923.594\n",
      "[852,    12] loss: 14875.071\n",
      "[853,    12] loss: 14910.473\n",
      "[854,    12] loss: 14892.312\n",
      "[855,    12] loss: 14884.597\n",
      "[856,    12] loss: 14965.677\n",
      "[857,    12] loss: 14900.622\n",
      "[858,    12] loss: 14921.346\n",
      "[859,    12] loss: 14935.067\n",
      "[860,    12] loss: 14885.726\n",
      "[861,    12] loss: 14907.116\n",
      "[862,    12] loss: 14910.573\n",
      "[863,    12] loss: 14895.348\n",
      "[864,    12] loss: 14891.750\n",
      "[865,    12] loss: 14901.618\n",
      "[866,    12] loss: 14917.965\n",
      "[867,    12] loss: 14885.711\n",
      "[868,    12] loss: 14907.464\n",
      "[869,    12] loss: 14905.674\n",
      "[870,    12] loss: 14870.772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[871,    12] loss: 14960.665\n",
      "[872,    12] loss: 14879.171\n",
      "[873,    12] loss: 14909.240\n",
      "[874,    12] loss: 14894.289\n",
      "[875,    12] loss: 14896.316\n",
      "[876,    12] loss: 14873.652\n",
      "[877,    12] loss: 14887.494\n",
      "[878,    12] loss: 14898.271\n",
      "[879,    12] loss: 14867.917\n",
      "[880,    12] loss: 14871.281\n",
      "[881,    12] loss: 14900.870\n",
      "[882,    12] loss: 14870.245\n",
      "[883,    12] loss: 14870.512\n",
      "[884,    12] loss: 14961.659\n",
      "[885,    12] loss: 14895.827\n",
      "[886,    12] loss: 14900.363\n",
      "[887,    12] loss: 14894.689\n",
      "[888,    12] loss: 14929.002\n",
      "[889,    12] loss: 14861.324\n",
      "[890,    12] loss: 14878.786\n",
      "[891,    12] loss: 14903.550\n",
      "[892,    12] loss: 14862.872\n",
      "[893,    12] loss: 14872.385\n",
      "[894,    12] loss: 14913.420\n",
      "[895,    12] loss: 14886.906\n",
      "[896,    12] loss: 14877.514\n",
      "[897,    12] loss: 14896.763\n",
      "[898,    12] loss: 14882.491\n",
      "[899,    12] loss: 14868.906\n",
      "[900,    12] loss: 14893.357\n",
      "[901,    12] loss: 14887.196\n",
      "[902,    12] loss: 14869.231\n",
      "[903,    12] loss: 14901.459\n",
      "[904,    12] loss: 14882.275\n",
      "[905,    12] loss: 14865.520\n",
      "[906,    12] loss: 14904.232\n",
      "[907,    12] loss: 14886.438\n",
      "[908,    12] loss: 14858.443\n",
      "[909,    12] loss: 14885.942\n",
      "[910,    12] loss: 14880.843\n",
      "[911,    12] loss: 14862.067\n",
      "[912,    12] loss: 14864.755\n",
      "[913,    12] loss: 14863.183\n",
      "[914,    12] loss: 14877.352\n",
      "[915,    12] loss: 14857.902\n",
      "[916,    12] loss: 14896.136\n",
      "[917,    12] loss: 14900.864\n",
      "[918,    12] loss: 14858.935\n",
      "[919,    12] loss: 14870.746\n",
      "[920,    12] loss: 14848.466\n",
      "[921,    12] loss: 14879.261\n",
      "[922,    12] loss: 14857.724\n",
      "[923,    12] loss: 14872.391\n",
      "[924,    12] loss: 14890.775\n",
      "[925,    12] loss: 14846.782\n",
      "[926,    12] loss: 14889.645\n",
      "[927,    12] loss: 14861.675\n",
      "[928,    12] loss: 14927.341\n",
      "[929,    12] loss: 14946.251\n",
      "[930,    12] loss: 14910.664\n",
      "[931,    12] loss: 14882.067\n",
      "[932,    12] loss: 14880.640\n",
      "[933,    12] loss: 14861.770\n",
      "[934,    12] loss: 14860.355\n",
      "[935,    12] loss: 14881.313\n",
      "[936,    12] loss: 14849.747\n",
      "[937,    12] loss: 14842.967\n",
      "[938,    12] loss: 14875.313\n",
      "[939,    12] loss: 14858.456\n",
      "[940,    12] loss: 14870.081\n",
      "[941,    12] loss: 14891.003\n",
      "[942,    12] loss: 14879.990\n",
      "[943,    12] loss: 14869.134\n",
      "[944,    12] loss: 14880.380\n",
      "[945,    12] loss: 14847.343\n",
      "[946,    12] loss: 14856.127\n",
      "[947,    12] loss: 14912.372\n",
      "[948,    12] loss: 14863.543\n",
      "[949,    12] loss: 14844.024\n",
      "[950,    12] loss: 14855.065\n",
      "[951,    12] loss: 14872.749\n",
      "[952,    12] loss: 14880.082\n",
      "[953,    12] loss: 14844.463\n",
      "[954,    12] loss: 14883.460\n",
      "[955,    12] loss: 14870.491\n",
      "[956,    12] loss: 14849.475\n",
      "[957,    12] loss: 14926.863\n",
      "[958,    12] loss: 14860.731\n",
      "[959,    12] loss: 14891.269\n",
      "[960,    12] loss: 14872.879\n",
      "[961,    12] loss: 14926.971\n",
      "[962,    12] loss: 14843.886\n",
      "[963,    12] loss: 14893.614\n",
      "[964,    12] loss: 14922.065\n",
      "[965,    12] loss: 14836.780\n",
      "[966,    12] loss: 14852.507\n",
      "[967,    12] loss: 14857.415\n",
      "[968,    12] loss: 14841.023\n",
      "[969,    12] loss: 14832.067\n",
      "[970,    12] loss: 14880.864\n",
      "[971,    12] loss: 14837.207\n",
      "[972,    12] loss: 14868.922\n",
      "[973,    12] loss: 14874.705\n",
      "[974,    12] loss: 14885.059\n",
      "[975,    12] loss: 14854.678\n",
      "[976,    12] loss: 14869.688\n",
      "[977,    12] loss: 14863.085\n",
      "[978,    12] loss: 14828.882\n",
      "[979,    12] loss: 14837.665\n",
      "[980,    12] loss: 14841.840\n",
      "[981,    12] loss: 14857.545\n",
      "[982,    12] loss: 14826.858\n",
      "[983,    12] loss: 14846.776\n",
      "[984,    12] loss: 14885.057\n",
      "[985,    12] loss: 14891.278\n",
      "[986,    12] loss: 14887.606\n",
      "[987,    12] loss: 14858.462\n",
      "[988,    12] loss: 14856.710\n",
      "[989,    12] loss: 14831.261\n",
      "[990,    12] loss: 14833.607\n",
      "[991,    12] loss: 14852.471\n",
      "[992,    12] loss: 14863.970\n",
      "[993,    12] loss: 14847.542\n",
      "[994,    12] loss: 14867.967\n",
      "[995,    12] loss: 14870.364\n",
      "[996,    12] loss: 14827.777\n",
      "[997,    12] loss: 14838.099\n",
      "[998,    12] loss: 14827.387\n",
      "[999,    12] loss: 14824.924\n",
      "[1000,    12] loss: 14858.798\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "modelo = modelo_NN()\n",
    "optimizer = torch.optim.Adagrad(modelo.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        xs, ys = data\n",
    "        \n",
    "        xs = xs.view(-1, 1)\n",
    "        ys = ys.view(-1, 1)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        modelo.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = modelo(xs)\n",
    "        \n",
    "        loss = criterion(outputs, ys)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Airline dataset')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "prediction = [modelo(torch.tensor([x_trn[i]])) for i in range(115)]\n",
    "plt.plot(tensor_x.numpy(),prediction)\n",
    "#plt.plot(tensor_x.numpy(),tensor_y.numpy())\n",
    "plt.plot(x_tst,y_tst, 'or')\n",
    "plt.plot(x_trn,y_trn, 'og')\n",
    "plt.title('Airline dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales\n",
    "\n",
    "Para definir una red en PyTorch en general se siguen los siguentes pasos.\n",
    "\n",
    "* Definir la red que tenga parámetros entrenables (pesos)\n",
    "* Iterar sobre el dataset de inputs\n",
    "* Procesas el input a través de la red\n",
    "* Calcular la función de perdida (loss)\n",
    "* Propagar los gradientes hacia los parametros de la red\n",
    "* Actualizar los pesos de la red siguiendo una regla, en general se usa gradiente decendiente.\n",
    "\n",
    "### Crear una red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(\"Capas:\", len(params))\n",
    "for i, param in enumerate(params):\n",
    "    print(\"Tamaño de parámetros de capa\", i+1, \"=>\", param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entregar una entrada aleatoria de tamaño 1x3x32x32. Es decir que un \"batch\" de un elemento, tres colores, altura 32 y ancho 32. Es `nn.Conv2d` acepta un tensor de quatro dimensiones de `nSamples x nChannels x Height x Width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3, 32, 32)\n",
    "out = net(input)\n",
    "print(out)  # tensor con tamaño 1x10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de perdida\n",
    "Una función de perdida toma una salida y el objetivo de su valor para calcular una medida de error. Por ejemplo, el `nn.MSELoss` que calcula el error de escuadrados mínimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos propagación hacia atrás para empotrar los valores de error en nuestros cadas de red neuronal. En PyTorch hacemos el \"backprop\" simplemente usando `backward()` en la salida de función de perdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()  # borrar los valores de antes de todos parámetros\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "net.conv1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualizar los pesos\n",
    "El método más simple de actualizar los pesos es el \"Stochastic Gradient Descent (SGD)\". En PyTorch podemos usar el packete `torch.optim` que nos da implementaciones de varios tipos de reglas de actualizar (como SGD, Nesterov-SGD, Adam, RMSProp, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal ejemplo\n",
    "Vamos a usar `torchvision` para cargar un base de datos de imágenes. Instala con `conda install torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos unos imágenes que tenemos en el set de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elejamos una función criteria y de perdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './cifar_net.pth') # guardar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos cuatro imágenes aleatorios para ver si la red funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print images\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos la red y verifiquemos la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tal funciona la red en los categorías?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "label_list = torch.zeros(0, dtype=torch.long)\n",
    "pred_list = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        label_list = torch.cat([label_list, labels.view(-1)])\n",
    "        pred_list = torch.cat([pred_list, predicted.view(-1)])\n",
    "        \n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat = confusion_matrix(label_list.numpy(), pred_list.numpy())\n",
    "df_cm = pd.DataFrame(conf_mat, index=classes, columns=classes)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sn.heatmap(df_cm, annot=True, cmap='rocket_r')\n",
    "\n",
    "b, t = plt.ylim()\n",
    "plt.ylim(b+0.5, t-0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entranar en el GPU\n",
    "Para verificar si tienes un GPU de Nvidia y has instalado CUDA (que es un método de acceder el GPU a través código), executamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el red en el GPU (si lo de arriba dice `cuda:0`), tenemos que mudar todo los datos y la red hacia el GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "inputs, labels = data[0].to(device), data[1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
